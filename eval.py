# -*- coding: utf-8 -*-
"""eval.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1na9QeKK3ZY3D8ieR_I5aw9HNeMEskZqn
"""

import os

try:
    import google.colab

    IN_COLAB = True
except:
    IN_COLAB = False
print(IN_COLAB)

#!pip install graph_nets  dm-sonnet tensorflow_probability

if IN_COLAB and 'drive' not in os.listdir("/content"):
    from google.colab import drive

    drive.mount('/content/drive')

if IN_COLAB:
    os.chdir('/content/drive/MyDrive/bakalarka')

import tensorflow as tf

import time
import sys
import re
import pickle

from graph_nets import utils_tf

import dataset

from common import NodeType
from dataset import load_dataset, add_targets, triangles_to_edges, prepare,split_dataset ,add_targets, add_batch

from EncodeProcessDecodeBasic import EncodeProcessDecode
from EncodeProcessDecodeNorm import EncodeProcessDecode as EncodeProcessDecodeNorm
from EncodeProcessDecodeAddNorm import EncodeProcessDecode as EncodeProcessDecodeAddNorm
from EncodeProcessDecodeNoLib import EncodeProcessDecode as EncodeProcessDecodeNoLib
from EncodeProcessDecodeMultinet import EncodeProcessDecode as EncodeProcessDecodeMultinet
from EncodeProcessDecodeNamedMultinet import EncodeProcessDecode as EncodeProcessDecodeNamedMultinet

data_path = 'data/airfoil'
data_train = 'small' if IN_COLAB else 'train'
data_valid = 'small' if IN_COLAB else 'valid'
data_test = 'small' if IN_COLAB else 'test'

mod = "named12n"
models = {"named": {"model": EncodeProcessDecodeNamedMultinet,
                    "args": {"steps": 15,
                             "learn_features": 2,
                             "n_layers": 2,
                             "lat_size": 128,
                             "edge_feat_cnt": 3,
                             "node_feat_cnt": 5},
                    "params": [dict(noise=0.02, gamma=1.0, field='velocity', history=False,
                                    size=2, batch=2)],
                    "path": "namednet"},
          "named18": {"model": EncodeProcessDecodeNamedMultinet,
                      "args": {"steps": 18,
                               "learn_features": 2,
                               "n_layers": 2,
                               "lat_size": 128,
                               "edge_feat_cnt": 3,
                               "node_feat_cnt": 5},
                      "params": [dict(noise=0.02, gamma=1.0, field='velocity', history=False,
                                      size=2, batch=2)],
                      "path": "namednet18"},
          "named12": {"model": EncodeProcessDecodeNamedMultinet,
                      "args": {"steps": 12,
                               "learn_features": 2,
                               "n_layers": 2,
                               "lat_size": 128,
                               "edge_feat_cnt": 3,
                               "node_feat_cnt": 5},
                      "params": [dict(noise=0.02, gamma=1.0, field='velocity', history=False,
                                      size=2, batch=2)],
                      "path": "namednet12"
                      },
          "named-full": {"model": EncodeProcessDecodeNamedMultinet,
                         "args": {"steps": 12,
                                  "learn_features": 4,
                                  "n_layers": 2,
                                  "lat_size": 128,
                                  "edge_feat_cnt": 3,
                                  "node_feat_cnt": 7},
                         "params": [dict(noise=0.02, gamma=1.0, field='velocity', history=False,
                                         size=2, batch=2),
                                    dict(noise=0.02, gamma=1.0, field='pressure', history=False,
                                         size=1, batch=2),
                                    dict(noise=0.02, gamma=1.0, field='density', history=False,
                                         size=1, batch=2)],
                         "path": "fullnet"},
          "named12n": {"model": EncodeProcessDecodeNamedMultinet,
                    "args": {"steps": 12,
                             "learn_features": 2,
                             "n_layers": 2,
                             "lat_size": 128,
                             "edge_feat_cnt": 3,
                             "node_feat_cnt": 5},
                    "params": [dict(noise=1.0, gamma=1.0, field='velocity', history=False,
                                    size=2, batch=2)],
                    "path": "namednet12n"}}

params = models[mod]["params"]
targets = tuple([x["field"] for x in params])


dt2 = load_dataset(data_path, data_test)
it = iter(dt2)
r = []
for i in range(10):
   r.append(it.next()['velocity'])
import pickle

with open('results/correct_path.pickle','wb') as x:
	pickle.dump(r,x)
exit()
dt2 = add_targets(dt2, targets, add_history=params[0]['history'])
qq = iter(dt2).next()

NodeTypeCnt = tf.unique(tf.reshape(qq['node_type'][0], qq['node_type'][0].shape[:1])).y.shape[0]

@tf.function
def toGraphsTuple(d, targets=('velocity',)):
    send, recive = triangles_to_edges(d['cells'])
    rel_pos = (tf.gather(d['mesh_pos'], send) - tf.gather(d['mesh_pos'], recive))
    nodes_unique = tf.unique_with_counts(tf.reshape(d["node_type"], [-1]))
    one_hot = tf.one_hot(nodes_unique.idx, NodeTypeCnt, dtype=tf.float32)
    dd = {
        "nodes": tf.concat([*[d[x] for x in targets], one_hot], 1),
        # on change update loss function ^
        "senders": send,
        "receivers": recive,
        "edges": tf.concat([
            rel_pos,
            tf.norm(rel_pos, axis=-1, keepdims=True)], 1)
    }
    return utils_tf.data_dicts_to_graphs_tuple([dd])

def get_init_grp(qq,targets=('velocity',)):
  qqq = {}
  for i, j in qq.items():
      qqq[i] = j[0]
  return toGraphsTuple(qqq, targets)

@tf.function
def rollout(model, grp_,length, loss_mask):
  res = [grp_, ]
  for i in range(length):
      grp2_ = model(grp_, False)
      grp_ = grp_.replace(nodes=tf.where(loss_mask, grp2_.nodes, grp_.nodes))
      res.append(grp_)
  return res

md = models[mod]
model = md["model"](**md["args"])
bsize = 10
mx_iter = 21

dtTrain = load_dataset(data_path,data_train)
dtValid = load_dataset(data_path,data_valid)
dtTrain = add_batch(prepare(split_dataset(add_targets(dtTrain,targets,False)),100 if IN_COLAB else 1000),bsize)

chck_root = "models/" + md["path"] + "/"
chck_name = mod

chck = tf.train.Checkpoint(module=model)

latest = tf.train.latest_checkpoint(chck_root)
if latest is not None:
    print("loading", latest)
    chck.restore(latest)

models_ = set([re.match('\w+-\w*-?\d+',x)[0] for x in filter(lambda x: x !="checkpoint", os.listdir(chck_root))])

itTrain = iter(dtTrain)
itValid = iter(dtValid)

kk = itValid.next()

loss_mask = tf.logical_or(tf.equal(kk['node_type'][0,:,0], NodeType.NORMAL),
                            tf.equal(kk['node_type'][0,:,0], NodeType.OUTFLOW))
loss_mask = tf.reshape(tf.concat([loss_mask for _ in range(md['args']['node_feat_cnt'])], -1), [-1, md['args']['node_feat_cnt']])

chck = tf.train.Checkpoint(module=model)
poss_steps = [x for x in [1, 10, 20, 50, 100, 200,600] if x <= mx_iter]
print(poss_steps)
stats = {}
losses = {}
print("start eval")
sys.stdout.flush()
for mid in models_:
  chck.restore(chck_root + mid)
  tr = itTrain.next()
  gr = toGraphsTuple(tr,targets)
  loss = model.loss(gr,tr,targets) / bsize
  losses[mid] = loss
  itValid = iter(dtValid)
  eval = {x:0 for x in poss_steps}
  for val in itValid:
    rr = rollout(model,get_init_grp(val,targets), mx_iter,loss_mask)
    pred = tf.concat([x.nodes[...,:model.learn_features] for x in rr], axis=0)
    correct = tf.concat([val[x] for x in targets],axis=2)
    pred = tf.reshape(pred,[mx_iter+1,*correct.shape[1:]])
    error = tf.reduce_mean((pred - correct[:mx_iter+1])**2, axis=-1)
    for x in poss_steps:
      eval[x] += tf.reduce_mean(error[1:x+1])
  stats[mid] = eval
  print(mid)
  sys.stdout.flush()

ref = [{"name": k,
 "i": int(re.search("\d+$",k)[0])*40000,
 "loss": losses[k].numpy(),
 "stat": {i: j.numpy() for i,j in v.items()}} for k,v in stats.items()]

with open("results/" + mod + ".pickle",'wb') as x:
  pickle.dump(ref,x)
